import asyncio
import logging
import sys
import gc
import psutil
import time
from pathlib import Path
import numpy as np

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/simplified_embeddings_generation.log'),
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)

from config_ac import settings
from process_documents import DocumentProcessor

SIMPLIFIED_CONFIG = {
    'chunk_batch_size': 32,        # Traite 32 chunks √† la fois
    'embedding_batch_size': 8,     # Embeddings par batch
    'memory_threshold_mb': 512,    # Pause si plus de 512MB utilis√©
    'gc_frequency': 50,            # Garbage collection tous les 50 chunks
    'progress_frequency': 25,      # Affiche progr√®s tous les 25 chunks
    'max_file_size_mb': 5,         # Limite la taille des fichiers trait√©s
    'pause_between_batches': 0.5   # Pause entre batches
}

# Configuration sp√©cifique pour le store am√©lior√©
ENHANCED_CONFIG = {
    'vector_store_path': 'vector_store_enhanced',
    'hybrid_search_enabled': True,
    'rerank_enabled': True,
    'query_expansion_enabled': True,
    'semantic_weight': 0.7,
    'lexical_weight': 0.3,
    'rerank_top_k': 20,
    'rerank_threshold': 0.5,
    'reranker_model': 'cross-encoder/ms-marco-MiniLM-L-6-v2',
    'embedding_model': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
    'diversity_threshold': 0.8,
    'max_context_length': 4000
}

def get_memory_usage():
    """Retourne l'utilisation m√©moire en MB"""
    process = psutil.Process()
    return process.memory_info().rss / 1024 / 1024

def memory_cleanup():
    """Nettoie la m√©moire"""
    gc.collect()
    if hasattr(gc, 'set_threshold'):
        gc.set_threshold(700, 10, 10)  

def monitor_memory():
    """Surveille la m√©moire et fait une pause si n√©cessaire"""
    memory_mb = get_memory_usage()
    
    if memory_mb > SIMPLIFIED_CONFIG['memory_threshold_mb']:
        logger.warning(f"‚ö†Ô∏è  M√©moire √©lev√©e: {memory_mb:.1f}MB - Nettoyage en cours...")
        memory_cleanup()
        time.sleep(2)
        
        new_memory = get_memory_usage()
        logger.info(f"üßπ M√©moire apr√®s nettoyage: {new_memory:.1f}MB")
    
    return memory_mb

def create_directories():
    """Cr√©e les dossiers n√©cessaires"""
    directories = [
        Path(ENHANCED_CONFIG['vector_store_path']).parent,
        Path("logs"),
        Path(settings.PLAN_CLIMAT_FOLDER),
        Path(settings.PRIORITY_MEASURES_FOLDER)
    ]
    
    for directory in directories:
        directory.mkdir(parents=True, exist_ok=True)
        logger.info(f"‚úÖ Dossier cr√©√©/v√©rifi√©: {directory}")

def validate_dependencies():
    """Valide que les d√©pendances pour le RAG sont install√©es"""
    logger.info("üîç Validation des d√©pendances...")
    
    missing_deps = []
    
    try:
        import sentence_transformers
        logger.info("‚úÖ sentence-transformers disponible")
    except ImportError:
        missing_deps.append("sentence-transformers")
    
    try:
        import sklearn
        logger.info("‚úÖ scikit-learn disponible")
    except ImportError:
        missing_deps.append("scikit-learn")
    
    try:
        import faiss
        logger.info("‚úÖ faiss disponible")
    except ImportError:
        missing_deps.append("faiss-cpu")
    
    if missing_deps:
        logger.error("‚ùå D√©pendances manquantes:")
        for dep in missing_deps:
            logger.error(f"   pip install {dep}")
        return False
    
    logger.info("‚úÖ Toutes les d√©pendances sont disponibles")
    return True

def validate_input_files():
    """Valide que tous les fichiers d'entr√©e sont pr√©sents"""
    logger.info("üîç Validation des fichiers d'entr√©e...")
    
    issues = []
    warnings = []
    
    # V√©rification du CSV
    csv_path = Path(settings.MUNICIPALITIES_CSV)
    if not csv_path.exists():
        issues.append(f"‚ùå Fichier CSV manquant: {csv_path}")
    else:
        logger.info(f"‚úÖ CSV trouv√©: {csv_path}")
    
    # V√©rification des PDFs Plan Climat
    plan_climat_folder = Path(settings.PLAN_CLIMAT_FOLDER)
    plan_climat_pdfs = list(plan_climat_folder.glob("*.pdf"))
    
    if not plan_climat_pdfs:
        issues.append(f"‚ùå Aucun PDF Plan Climat trouv√© dans: {plan_climat_folder}")
    else:
        logger.info(f"‚úÖ {len(plan_climat_pdfs)} PDFs Plan Climat trouv√©s")
        
        for pdf in plan_climat_pdfs:
            size_mb = pdf.stat().st_size / (1024 * 1024)
            if size_mb > SIMPLIFIED_CONFIG['max_file_size_mb']:
                warnings.append(f"‚ö†Ô∏è  Fichier volumineux: {pdf.name} ({size_mb:.1f}MB)")
            logger.info(f"   {pdf.name}: {size_mb:.1f}MB")
    
    # V√©rification des PDFs Mesures
    mesures_folder = Path(settings.PRIORITY_MEASURES_FOLDER)
    mesures_pdfs = list(mesures_folder.glob("*.pdf"))
    
    if not mesures_pdfs:
        issues.append(f"‚ùå Aucun PDF Mesures trouv√© dans: {mesures_folder}")
    else:
        logger.info(f"‚úÖ {len(mesures_pdfs)} PDFs Mesures trouv√©s")
        
        for pdf in mesures_pdfs:
            size_mb = pdf.stat().st_size / (1024 * 1024)
            if size_mb > SIMPLIFIED_CONFIG['max_file_size_mb']:
                warnings.append(f"‚ö†Ô∏è  Fichier volumineux: {pdf.name} ({size_mb:.1f}MB)")
            logger.info(f"   {pdf.name}: {size_mb:.1f}MB")
    
    if warnings:
        logger.warning("‚ö†Ô∏è  Fichiers volumineux d√©tect√©s - peuvent causer des probl√®mes de m√©moire")
        for warning in warnings:
            logger.warning(f"   {warning}")
    
    if issues:
        logger.error("‚ùå Probl√®mes d√©tect√©s:")
        for issue in issues:
            logger.error(f"   {issue}")
        return False
    
    logger.info("‚úÖ Tous les fichiers requis sont pr√©sents")
    return True

def process_documents():
    """Traite les documents"""
    logger.info("üìÑ Traitement des documents...")
    
    try:
        processor = DocumentProcessor(settings)
        
        # Chargement des donn√©es des communes
        logger.info("Chargement des donn√©es des communes...")
        municipalities = processor.load_municipalities_data()
        logger.info(f"‚úÖ {len(municipalities)} communes charg√©es")
        
        all_chunks = []
        
        # Traitement des Mesures
        logger.info("üéØ Traitement des Mesures Prioritaires...")
        mesures_chunks = processor.process_priority_measures_documents()
        all_chunks.extend(mesures_chunks)
        logger.info(f"   Mesures: {len(mesures_chunks)} chunks cr√©√©s")
        
        # Nettoyage m√©moire
        memory_cleanup()
        
        # Traitement des Plans Climat
        logger.info("üìã Traitement des Plans Climat...")
        plan_climat_chunks = processor.process_plan_climat_documents()
        all_chunks.extend(plan_climat_chunks)
        logger.info(f"   Plans Climat: {len(plan_climat_chunks)} chunks cr√©√©s")
        
        # Nettoyage m√©moire final
        memory_cleanup()
        
        if not all_chunks:
            raise ValueError("Aucun chunk cr√©√© - v√©rifiez vos fichiers PDF")
        
        logger.info(f"‚úÖ {len(all_chunks)} chunks cr√©√©s au total")
        return all_chunks
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du traitement des documents: {e}")
        raise

def create_vector_store():
    """Cr√©e le store vectoriel"""
    logger.info("üöÄ Cr√©ation du store vectoriel...")
    
    # Import dynamique pour √©viter les erreurs si pas install√©
    try:
        from enhanced_vector_store import EnhancedVectorStore
    except ImportError:
        logger.error("‚ùå enhanced_vector_store.py non trouv√©")
        raise
    
    # Configuration compl√®te pour le store 
    enhanced_settings = {
        'index_path': ENHANCED_CONFIG['vector_store_path'],
        **ENHANCED_CONFIG
    }
    
    if hasattr(settings, '__dict__'):
        for key, value in settings.__dict__.items():
            if not key.startswith('_') and key not in enhanced_settings:
                enhanced_settings[key] = value
    
    vector_store = EnhancedVectorStore(enhanced_settings)
    logger.info("‚úÖ Store vectoriel cr√©√©")
    
    return vector_store

def generate_embeddings_with_simplified_store(chunks):
    """G√©n√®re les embeddings avec le store"""
    logger.info("üîç G√©n√©ration des embeddings avec store...")
    
    try:
        vector_store = create_vector_store()
        
        start_time = time.time()
        total_chunks = len(chunks)
        
        logger.info(f"G√©n√©ration pour {total_chunks} chunks")
        logger.info(f"Configuration: batch_size={SIMPLIFIED_CONFIG['embedding_batch_size']}")
        logger.info(f"Fonctionnalit√©s: hybrid={ENHANCED_CONFIG['hybrid_search_enabled']}, "
                   f"rerank={ENHANCED_CONFIG['rerank_enabled']}")
        
        # Pr√©pare les donn√©es des chunks pour le store 
        chunk_data = []
        for chunk in chunks:
            chunk_dict = {
                'chunk_id': chunk.chunk_id,
                'content': chunk.content,
                'source_file': chunk.source_file,
                'page_number': chunk.page_number,
                'document_type': chunk.document_type,
                'metadata': chunk.metadata
            }
            chunk_data.append(chunk_dict)
        
        # G√©n√©ration des embeddings par batch
        logger.info("ü§ñ G√©n√©ration des embeddings...")
        batch_size = SIMPLIFIED_CONFIG['embedding_batch_size']
        processed = 0
        
        for i in range(0, total_chunks, batch_size):
            batch_chunks = chunk_data[i:i + batch_size]
            batch_texts = [chunk['content'] for chunk in batch_chunks]
            
            logger.info(f"   Batch {i//batch_size + 1}/{(total_chunks-1)//batch_size + 1} ({len(batch_texts)} chunks)")
            
            # G√©n√©ration des embeddings pour ce batch
            batch_embeddings = vector_store.create_embeddings(batch_texts)
            
            # Ajoute les embeddings aux chunks
            for j, embedding in enumerate(batch_embeddings):
                chunk_idx = i + j
                if chunk_idx < len(chunk_data):
                    chunk_data[chunk_idx]['embedding'] = embedding
            
            processed += len(batch_texts)
            
            # Monitoring m√©moire
            if processed % SIMPLIFIED_CONFIG['progress_frequency'] == 0:
                memory_mb = monitor_memory()
                progress = (processed / total_chunks) * 100
                elapsed = time.time() - start_time
                eta = (elapsed / processed) * (total_chunks - processed) if processed > 0 else 0
                
                logger.info(f"   Progr√®s: {processed}/{total_chunks} ({progress:.1f}%) - "
                          f"M√©moire: {memory_mb:.1f}MB - ETA: {eta:.1f}s")
            
            # Nettoyage p√©riodique
            if processed % SIMPLIFIED_CONFIG['gc_frequency'] == 0:
                memory_cleanup()
            
            # Pause syst√®me
            time.sleep(SIMPLIFIED_CONFIG['pause_between_batches'])
        
        # Construction de l'index 
        logger.info("üèóÔ∏è  Construction de l'index...")
        vector_store.build_index(chunk_data)
        
        embedding_time = time.time() - start_time
        logger.info(f"‚úÖ Index cr√©√© en {embedding_time:.1f} secondes")
        
        logger.info("üíæ Sauvegarde de l'index vectoriel...")
        vector_store.save_index()
        
        # Statistiques finales
        stats = vector_store.get_statistics()
        logger.info("üìä Statistiques de l'index:")
        logger.info(f"   Total chunks: {stats['total_chunks']}")
        logger.info(f"   Index type: {stats.get('index_type', 'Enhanced')}")
        logger.info(f"   Hybrid search: {stats.get('hybrid_search', False)}")
        logger.info(f"   Reranking: {stats.get('reranking_enabled', False)}")
        logger.info(f"   Cantons: {len(vector_store.get_available_cantons())}")
        logger.info(f"   Secteurs: {len(vector_store.get_available_sectors())}")
        
        return vector_store
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la g√©n√©ration: {e}")
        raise

def validate_simplified_embeddings():
    """Valide que les embeddings ont √©t√© correctement sauvegard√©s"""
    logger.info("üß™ Validation de l'index...")
    
    try:
        from enhanced_vector_store import EnhancedVectorStore
        
        # Configuration pour le store simplifi√©
        enhanced_settings = {
            'index_path': ENHANCED_CONFIG['vector_store_path'],
            **ENHANCED_CONFIG
        }
        
        vector_store = EnhancedVectorStore(enhanced_settings)
        
        # Test de chargement
        if vector_store.load_index():
            stats = vector_store.get_statistics()
            logger.info(f"‚úÖ Index charg√©: {stats['total_chunks']} chunks")
            
            # Validation basique de la structure de l'index
            if hasattr(vector_store, 'index') and vector_store.index is not None:
                logger.info("‚úÖ Index FAISS charg√© correctement")
            
            if hasattr(vector_store, 'chunks') and vector_store.chunks:
                logger.info(f"‚úÖ {len(vector_store.chunks)} chunks de donn√©es charg√©s")
            
            # Test des mappings de m√©tadonn√©es
            if hasattr(vector_store, 'canton_mapping') and vector_store.canton_mapping:
                cantons_count = len(vector_store.canton_mapping)
                logger.info(f"‚úÖ Mappings canton: {cantons_count} cantons")
            
            if hasattr(vector_store, 'sector_mapping') and vector_store.sector_mapping:
                sectors_count = len(vector_store.sector_mapping)
                logger.info(f"‚úÖ Mappings secteur: {sectors_count} secteurs")
            
            # Test des fonctionnalit√©s avanc√©es
            if stats.get('hybrid_search'):
                logger.info("‚úÖ Recherche hybride activ√©e")
                
                # V√©rifie l'index TF-IDF
                if (hasattr(vector_store.hybrid_engine, 'tfidf_vectorizer') and 
                    vector_store.hybrid_engine.tfidf_vectorizer is not None):
                    logger.info("‚úÖ Index TF-IDF charg√©")
                
                if (hasattr(vector_store.hybrid_engine, 'reranker') and 
                    vector_store.hybrid_engine.reranker is not None):
                    logger.info("‚úÖ Mod√®le de reranking charg√©")
            
            # Test simple de recherche
            try:
                test_results = vector_store.search("transport", k=3, method="semantic")
                if test_results:
                    logger.info(f"‚úÖ Test recherche s√©mantique: {len(test_results)} r√©sultats")
                
                # Test de recherche hybride si disponible
                if stats.get('hybrid_search'):
                    hybrid_results = vector_store.search("efficacit√© √©nerg√©tique", k=3, method="hybrid")
                    if hybrid_results:
                        logger.info(f"‚úÖ Test recherche hybride: {len(hybrid_results)} r√©sultats")
                        
                        # V√©rifie les scores avanc√©s
                        first_result = hybrid_results[0]
                        has_advanced_scores = any(key in first_result for key in 
                                                ['semantic_score', 'lexical_score', 'rerank_score'])
                        if has_advanced_scores:
                            logger.info("‚úÖ Scores avanc√©s d√©tect√©s dans les r√©sultats")
                
            except Exception as search_error:
                logger.warning(f"‚ö†Ô∏è  Tests de recherche partiellement √©chou√©s: {search_error}")
                logger.info("‚úÖ Index valid√© (structure correcte, recherche √† tester en usage)")
            
            # Test de recherche avec m√©tadonn√©es
            try:
                cantons = vector_store.get_available_cantons()
                if cantons and len(cantons) > 0:
                    logger.info(f"‚úÖ {len(cantons)} cantons disponibles: {cantons[:3]}...")
                
                sectors = vector_store.get_available_sectors()
                if sectors and len(sectors) > 0:
                    logger.info(f"‚úÖ {len(sectors)} secteurs disponibles: {sectors[:3]}...")
                    
            except Exception as metadata_error:
                logger.warning(f"‚ö†Ô∏è  Test m√©tadonn√©es √©chou√©: {metadata_error}")
            
            logger.info("‚úÖ Validation de l'index termin√©e avec succ√®s")
            return True
        else:
            logger.error("‚ùå Impossible de charger l'index")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la validation: {e}")
        return False

def main():
    """Fonction principale pour g√©n√©rer l'index simplifi√©"""
    logger.info("üöÄ G√©n√©ration SIMPLIFI√âE des embeddings pour l'Assistant PECC")
    logger.info("=" * 70)
    
    # Affichage de la configuration
    logger.info("‚öôÔ∏è  Configuration:")
    for key, value in SIMPLIFIED_CONFIG.items():
        logger.info(f"   {key}: {value}")
    
    logger.info("üî¨ Configuration avanc√©e:")
    for key, value in ENHANCED_CONFIG.items():
        logger.info(f"   {key}: {value}")
    
    # Monitoring initial
    initial_memory = get_memory_usage()
    logger.info(f"üíæ M√©moire initiale: {initial_memory:.1f}MB")
    
    start_total = time.time()
    
    try:
        # 1. Validation des d√©pendances
        logger.info("\n√âtape 1/6: Validation des d√©pendances")
        if not validate_dependencies():
            logger.error("‚ùå D√©pendances manquantes - installez-les avec:")
            logger.error("pip install sentence-transformers scikit-learn faiss-cpu")
            return False
        
        # 2. Cr√©ation des dossiers
        logger.info("\n√âtape 2/6: Cr√©ation des dossiers")
        create_directories()
        
        # 3. Validation des fichiers
        logger.info("\n√âtape 3/6: Validation des fichiers d'entr√©e")
        if not validate_input_files():
            logger.error("‚ùå Validation √©chou√©e - v√©rifiez vos fichiers")
            return False
        
        # 4. Traitement des documents
        logger.info("\n√âtape 4/6: Traitement des documents")
        chunks = process_documents()
        
        # 5. G√©n√©ration des embeddings simplifi√©s
        logger.info("\n√âtape 5/6: G√©n√©ration des embeddings simplifi√©s")
        vector_store = generate_embeddings_with_simplified_store(chunks)
        
        # 6. Validation de l'index
        logger.info("\n√âtape 6/6: Validation de l'index")
        if not validate_simplified_embeddings():
            logger.error("‚ùå Validation de l'index √©chou√©e")
            return False
        
        # R√©sum√© final
        total_time = time.time() - start_total
        final_memory = get_memory_usage()
        
        logger.info("\n" + "=" * 70)
        logger.info("üéâ G√©n√©ration SIMPLIFI√âE termin√©e avec succ√®s!")
        logger.info(f"‚è±Ô∏è  Temps total: {total_time:.1f} secondes")
        logger.info(f"üíæ M√©moire finale: {final_memory:.1f}MB (vs {initial_memory:.1f}MB initial)")
        logger.info(f"üìÅ Index sauv√©: {ENHANCED_CONFIG['vector_store_path']}")
        
        logger.info("\nüöÄ Fonctionnalit√©s activ√©es:")
        logger.info(f"   ‚úÖ Recherche hybride (s√©mantique + lexicale)")
        logger.info(f"   ‚úÖ Reranking avec cross-encoder")
        logger.info(f"   ‚úÖ Expansion de requ√™te")
        logger.info(f"   ‚úÖ Embeddings int√©gr√©s")
        logger.info(f"   ‚úÖ Filtrage par m√©tadonn√©es")
        
        logger.info("\nüìã Prochaines √©tapes:")
        logger.info("   1. Lancez l'assistant: streamlit run simplified_pecc_assistant.py")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erreur fatale: {e}")
        return False

if __name__ == "__main__":
    # V√©rification des ressources syst√®me
    total_memory_gb = psutil.virtual_memory().total / (1024**3)
    available_memory_gb = psutil.virtual_memory().available / (1024**3)
    
    logger.info(f"üíæ M√©moire syst√®me: {available_memory_gb:.1f}GB disponible / {total_memory_gb:.1f}GB total")
    
    # Adaptation automatique selon la m√©moire disponible
    if total_memory_gb < 4:
        logger.warning(f"‚ö†Ô∏è  M√©moire faible: {total_memory_gb:.1f}GB - Mode ultra-l√©ger")
        SIMPLIFIED_CONFIG['embedding_batch_size'] = 2
        SIMPLIFIED_CONFIG['memory_threshold_mb'] = 256
        ENHANCED_CONFIG['rerank_enabled'] = False  # D√©sactive reranking sur machines faibles
        logger.warning("   Reranking d√©sactiv√© pour √©conomiser la m√©moire")
    elif total_memory_gb < 8:
        logger.info(f"üìä M√©moire mod√©r√©e: {total_memory_gb:.1f}GB - Mode l√©ger")
        SIMPLIFIED_CONFIG['embedding_batch_size'] = 4
        SIMPLIFIED_CONFIG['memory_threshold_mb'] = 512
    else:
        logger.info(f"‚úÖ M√©moire suffisante: {total_memory_gb:.1f}GB - Mode normal")
        SIMPLIFIED_CONFIG['embedding_batch_size'] = 8
        SIMPLIFIED_CONFIG['memory_threshold_mb'] = 1024
    
    success = main()
    sys.exit(0 if success else 1)